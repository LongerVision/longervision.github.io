<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Longer Vision</title>
  <meta name="author" content="Nobody">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Longer Vision"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Longer Vision" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Longer Vision</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-20T10:29:15.000Z"><a href="/2017/03/20/opencv-internal-calibration-circle-grid/">2017-03-20</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/20/opencv-internal-calibration-circle-grid/">Camera Calibration Using a Circle Grid</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>By reading two of our previous blogs <a href="https://longervision.github.io/2017/03/19/opencv-internal-calibration-chessboard/">Camera Calibration Using a Chessboard</a> and <a href="https://longervision.github.io/2017/03/13/opencv-external-posture-estimation-circle-grid/">Camera Posture Estimation Using Circle Grid Pattern</a>, it wouldn’t be hard to replace the chessboard by a circle grid to calculate camera calibration parameters.<p></p>
<p></p><h2>Coding</h2><br>Our code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/01_internal_camera_calibration/circle_grid.py" target="_blank" rel="external">OpenCV Examples</a>.<p></p>
<p></p><h3>First of all</h3><br>As mentioned in <a href="https://longervision.github.io/2017/03/19/opencv-internal-calibration-chessboard/">Camera Calibration Using a Chessboard</a>, for intrinsic parameters estimation, namely, camera calibration, there is <strong>NO</strong> need to measure the circle unit size. And the circle gird is to be adopted is exactly the same one as used in <a href="https://longervision.github.io/2017/03/13/opencv-external-posture-estimation-circle-grid/">Camera Posture Estimation Using Circle Grid Pattern</a>:<br><img src="https://raw.githubusercontent.com/LongerVision/OpenCV_Examples/master/markers/pattern_acircles.png" alt="asymmetric_circle_grid" title="asymmetric_circle_grid"><p></p>
<p></p><h3>Secondly</h3><br>Required packages need to be imported.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import cv2</div><div class="line">import yaml</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Thirdly</h3><br>Some initialization work need to be done, including: 1) define the termination criteria when refine the corner sub-pixel later on; 2) blob detection parameters; 3) object points coordinators initialization.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div></pre></td><td class="code"><pre><div class="line"># termination criteria</div><div class="line">criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)</div><div class="line"></div><div class="line">########################################Blob Detector##############################################</div><div class="line"># Setup SimpleBlobDetector parameters.</div><div class="line">blobParams = cv2.SimpleBlobDetector_Params()</div><div class="line"></div><div class="line"># Change thresholds</div><div class="line">blobParams.minThreshold = 8</div><div class="line">blobParams.maxThreshold = 255</div><div class="line"></div><div class="line"># Filter by Area.</div><div class="line">blobParams.filterByArea = True</div><div class="line">blobParams.minArea = 64     # minArea may be adjusted to suit for your experiment</div><div class="line">blobParams.maxArea = 2500   # maxArea may be adjusted to suit for your experiment</div><div class="line"></div><div class="line"># Filter by Circularity</div><div class="line">blobParams.filterByCircularity = True</div><div class="line">blobParams.minCircularity = 0.1</div><div class="line"></div><div class="line"># Filter by Convexity</div><div class="line">blobParams.filterByConvexity = True</div><div class="line">blobParams.minConvexity = 0.87</div><div class="line"></div><div class="line"># Filter by Inertia</div><div class="line">blobParams.filterByInertia = True</div><div class="line">blobParams.minInertiaRatio = 0.01</div><div class="line"></div><div class="line"># Create a detector with the parameters</div><div class="line">blobDetector = cv2.SimpleBlobDetector_create(blobParams)</div><div class="line">###################################################################################################</div><div class="line"></div><div class="line"></div><div class="line">###################################################################################################</div><div class="line"># Original blob coordinates, supposing all blobs are of z-coordinates 0</div><div class="line"># And, the distance between every two neighbour blob circle centers is 72 centimetres</div><div class="line"># In fact, any number can be used to replace 72.</div><div class="line"># Namely, the real size of the circle is pointless while calculating camera calibration parameters.</div><div class="line">objp = np.zeros((44, 3), np.float32)</div><div class="line">objp[0]  = (0  , 0  , 0)</div><div class="line">objp[1]  = (0  , 72 , 0)</div><div class="line">objp[2]  = (0  , 144, 0)</div><div class="line">objp[3]  = (0  , 216, 0)</div><div class="line">objp[4]  = (36 , 36 , 0)</div><div class="line">objp[5]  = (36 , 108, 0)</div><div class="line">objp[6]  = (36 , 180, 0)</div><div class="line">objp[7]  = (36 , 252, 0)</div><div class="line">objp[8]  = (72 , 0  , 0)</div><div class="line">objp[9]  = (72 , 72 , 0)</div><div class="line">objp[10] = (72 , 144, 0)</div><div class="line">objp[11] = (72 , 216, 0)</div><div class="line">objp[12] = (108, 36,  0)</div><div class="line">objp[13] = (108, 108, 0)</div><div class="line">objp[14] = (108, 180, 0)</div><div class="line">objp[15] = (108, 252, 0)</div><div class="line">objp[16] = (144, 0  , 0)</div><div class="line">objp[17] = (144, 72 , 0)</div><div class="line">objp[18] = (144, 144, 0)</div><div class="line">objp[19] = (144, 216, 0)</div><div class="line">objp[20] = (180, 36 , 0)</div><div class="line">objp[21] = (180, 108, 0)</div><div class="line">objp[22] = (180, 180, 0)</div><div class="line">objp[23] = (180, 252, 0)</div><div class="line">objp[24] = (216, 0  , 0)</div><div class="line">objp[25] = (216, 72 , 0)</div><div class="line">objp[26] = (216, 144, 0)</div><div class="line">objp[27] = (216, 216, 0)</div><div class="line">objp[28] = (252, 36 , 0)</div><div class="line">objp[29] = (252, 108, 0)</div><div class="line">objp[30] = (252, 180, 0)</div><div class="line">objp[31] = (252, 252, 0)</div><div class="line">objp[32] = (288, 0  , 0)</div><div class="line">objp[33] = (288, 72 , 0)</div><div class="line">objp[34] = (288, 144, 0)</div><div class="line">objp[35] = (288, 216, 0)</div><div class="line">objp[36] = (324, 36 , 0)</div><div class="line">objp[37] = (324, 108, 0)</div><div class="line">objp[38] = (324, 180, 0)</div><div class="line">objp[39] = (324, 252, 0)</div><div class="line">objp[40] = (360, 0  , 0)</div><div class="line">objp[41] = (360, 72 , 0)</div><div class="line">objp[42] = (360, 144, 0)</div><div class="line">objp[43] = (360, 216, 0)</div><div class="line">###################################################################################################</div><div class="line"></div><div class="line"></div><div class="line"># Arrays to store object points and image points from all the images.</div><div class="line">objpoints = [] # 3d point in real world space</div><div class="line">imgpoints = [] # 2d points in image plane.</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Fourthly</h3><br>After localizing 10 frames (<strong>10</strong> can be changed to any positive integer as you wish) of a grid of 2D chessboard cell corners, camera matrix and distortion coefficients can be calculated by invoking the function <strong>calibrateCamera</strong>. Here, we are testing on a real-time camera stream.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">cap = cv2.VideoCapture(0)</div><div class="line">found = 0</div><div class="line">while(found &lt; 10):  # Here, 10 can be changed to whatever number you like to choose</div><div class="line">    ret, img = cap.read() # Capture frame-by-frame</div><div class="line">    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</div><div class="line"></div><div class="line">    keypoints = blobDetector.detect(gray) # Detect blobs.</div><div class="line"></div><div class="line">    # Draw detected blobs as red circles. This helps cv2.findCirclesGrid() .</div><div class="line">    im_with_keypoints = cv2.drawKeypoints(img, keypoints, np.array([]), (0,255,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)</div><div class="line">    im_with_keypoints_gray = cv2.cvtColor(im_with_keypoints, cv2.COLOR_BGR2GRAY)</div><div class="line">    ret, corners = cv2.findCirclesGrid(im_with_keypoints, (4,11), None, flags = cv2.CALIB_CB_ASYMMETRIC_GRID)   # Find the circle grid</div><div class="line"></div><div class="line">    if ret == True:</div><div class="line">        objpoints.append(objp)  # Certainly, every loop objp is the same, in 3D.</div><div class="line"></div><div class="line">        corners2 = cv2.cornerSubPix(im_with_keypoints_gray, corners, (11,11), (-1,-1), criteria)    # Refines the corner locations.</div><div class="line">        imgpoints.append(corners2)</div><div class="line"></div><div class="line">        # Draw and display the corners.</div><div class="line">        im_with_keypoints = cv2.drawChessboardCorners(img, (4,11), corners2, ret)</div><div class="line">        found += 1</div><div class="line"></div><div class="line"></div><div class="line">    cv2.imshow(&quot;img&quot;, im_with_keypoints) # display</div><div class="line">    cv2.waitKey(2)</div><div class="line"></div><div class="line"></div><div class="line"># When everything done, release the capture</div><div class="line">cap.release()</div><div class="line">cv2.destroyAllWindows()</div><div class="line"></div><div class="line">ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Finally</h3><br>Write the calculated calibration parameters into a yaml file. Here, it is a bit tricky. Please bear in mind that you <strong>MUST</strong> call function <strong>tolist()</strong> to transform a numpy array to a list.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># It&apos;s very important to transform the matrix to list.</div><div class="line">data = &#123;&apos;camera_matrix&apos;: np.asarray(mtx).tolist(), &apos;dist_coeff&apos;: np.asarray(dist).tolist()&#125;</div><div class="line"></div><div class="line">with open(&quot;calibration.yaml&quot;, &quot;w&quot;) as f:</div><div class="line">    yaml.dump(data, f)</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Additionally</h3><br>You may use the following piece of code to load the calibration parameters from file “calibration.yaml”.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">with open(&apos;calibration.yaml&apos;) as f:</div><div class="line">    loadeddict = yaml.load(f)</div><div class="line">mtxloaded = loadeddict.get(&apos;camera_matrix&apos;)</div><div class="line">distloaded = loadeddict.get(&apos;dist_coeff&apos;)</div></pre></td></tr></table></figure><p></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-19T21:48:05.000Z"><a href="/2017/03/19/opencv-internal-calibration-chessboard/">2017-03-19</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/19/opencv-internal-calibration-chessboard/">Camera Calibration Using a Chessboard</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>Traditionally, a camera is calibrated using a chessboard. Existing documentations are already out there and have discussed camera calibration in detail, for example, <a href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html" target="_blank" rel="external">OpenCV-Python Tutorials</a>.<p></p>
<p></p><h2>Coding</h2><br>Our code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/01_internal_camera_calibration/chessboard.py" target="_blank" rel="external">OpenCV Examples</a>.<p></p>
<p></p><h3>First of all</h3><br>Unlike estimating camera postures which is dealing with the extrinsic parameters, camera calibration is to calculate the intrinsic parameters. In such a case, there is <strong>NO</strong> need for us to measure the cell size of the chessboard. Anyway, the adopted chessboard is just an ordinary chessboard as:<br><img src="https://raw.githubusercontent.com/LongerVision/OpenCV_Examples/master/markers/pattern_chessboard.png" alt="pattern_chessboard" title="pattern_chessboard"><p></p>
<p></p><h3>Secondly</h3><br>Required packages need to be imported.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import cv2</div><div class="line">import yaml</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Thirdly</h3><br>Some initialization work need to be done, including: 1) define the termination criteria when refine the corner sub-pixel later on; 2) object points coordinators initialization.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># termination criteria</div><div class="line">criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)</div><div class="line"></div><div class="line"># prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)</div><div class="line">objp = np.zeros((6*7,3), np.float32)</div><div class="line">objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)</div><div class="line"></div><div class="line"># Arrays to store object points and image points from all the images.</div><div class="line">objpoints = [] # 3d point in real world space</div><div class="line">imgpoints = [] # 2d points in image plane.</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Fourthly</h3><br>After localizing 10 frames (<strong>10</strong> can be changed to any positive integer as you wish) of a grid of 2D chessboard cell corners, camera matrix and distortion coefficients can be calculated by invoking the function <strong>calibrateCamera</strong>. Here, we are testing on a real-time camera stream.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">cap = cv2.VideoCapture(0)</div><div class="line">found = 0</div><div class="line">while(found &lt; 10):  # Here, 10 can be changed to whatever number you like to choose</div><div class="line">    ret, img = cap.read() # Capture frame-by-frame</div><div class="line">    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</div><div class="line"></div><div class="line">    # Find the chess board corners</div><div class="line">    ret, corners = cv2.findChessboardCorners(gray, (7,6),None)</div><div class="line"></div><div class="line">    # If found, add object points, image points (after refining them)</div><div class="line">    if ret == True:</div><div class="line">        objpoints.append(objp)   # Certainly, every loop objp is the same, in 3D.</div><div class="line"></div><div class="line">        corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)</div><div class="line">        imgpoints.append(corners2)</div><div class="line"></div><div class="line">        # Draw and display the corners</div><div class="line">        img = cv2.drawChessboardCorners(img, (7,6), corners2, ret)</div><div class="line">        found += 1</div><div class="line"></div><div class="line">    cv2.imshow(&apos;img&apos;, img)</div><div class="line">    cv2.waitKey(10)</div><div class="line"></div><div class="line"></div><div class="line"># When everything done, release the capture</div><div class="line">cap.release()</div><div class="line">cv2.destroyAllWindows()</div><div class="line"></div><div class="line">ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Finally</h3><br>Write the calculated calibration parameters into a yaml file. Here, it is a bit tricky. Please bear in mind that you <strong>MUST</strong> call function <strong>tolist()</strong> to transform a numpy array to a list.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># It&apos;s very important to transform the matrix to list.</div><div class="line">data = &#123;&apos;camera_matrix&apos;: np.asarray(mtx).tolist(), &apos;dist_coeff&apos;: np.asarray(dist).tolist()&#125;</div><div class="line"></div><div class="line">with open(&quot;calibration.yaml&quot;, &quot;w&quot;) as f:</div><div class="line">    yaml.dump(data, f)</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Additionally</h3><br>You may use the following piece of code to load the calibration parameters from file “calibration.yaml”.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">with open(&apos;calibration.yaml&apos;) as f:</div><div class="line">    loadeddict = yaml.load(f)</div><div class="line">mtxloaded = loadeddict.get(&apos;camera_matrix&apos;)</div><div class="line">distloaded = loadeddict.get(&apos;dist_coeff&apos;)</div></pre></td></tr></table></figure><p></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T22:49:11.000Z"><a href="/2017/03/13/opencv-python-aruco/">2017-03-13</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/13/opencv-python-aruco/">OpenCV Python ArUco Documentation</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Open a bash terminal, and type in the following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">Python 3.5.2 (default, Nov 17 2016, 17:05:23)</div><div class="line">[GCC 5.4.0 20160609] on linux</div><div class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</div><div class="line">&gt;&gt;&gt; import cv2</div><div class="line">&gt;&gt;&gt; <span class="built_in">help</span> (cv2.aruco)</div></pre></td></tr></table></figure></p>
<p>Then, you will be able to see all the doc contents for <strong>cv2.aruco</strong>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line">Help on module cv2.aruco in cv2:</div><div class="line"></div><div class="line">NAME</div><div class="line">    cv2.aruco</div><div class="line"></div><div class="line">FUNCTIONS</div><div class="line">    Board_create(...)</div><div class="line">        Board_create(objPoints, dictionary, ids) -&gt; retval</div><div class="line"></div><div class="line">    CharucoBoard_create(...)</div><div class="line">        CharucoBoard_create(squaresX, squaresY, squareLength, markerLength, dictionary) -&gt; retval</div><div class="line"></div><div class="line">    DetectorParameters_create(...)</div><div class="line">        DetectorParameters_create() -&gt; retval</div><div class="line"></div><div class="line">    Dictionary_create(...)</div><div class="line">        Dictionary_create(nMarkers, markerSize) -&gt; retval</div><div class="line"></div><div class="line">    Dictionary_create_from(...)</div><div class="line">        Dictionary_create_from(nMarkers, markerSize, baseDictionary) -&gt; retval</div><div class="line"></div><div class="line">    Dictionary_get(...)</div><div class="line">        Dictionary_get(dict) -&gt; retval</div><div class="line"></div><div class="line">    GridBoard_create(...)</div><div class="line">        GridBoard_create(markersX, markersY, markerLength, markerSeparation, dictionary[, firstMarker]) -&gt; retval</div><div class="line"></div><div class="line">    calibrateCameraAruco(...)</div><div class="line">        calibrateCameraAruco(corners, ids, counter, board, imageSize, cameraMatrix, distCoeffs[, rvecs[, tvecs[, flags[, criteria]]]]) -&gt; retval, cameraMatrix, distCoeffs, rvecs, tvecs</div><div class="line"></div><div class="line">    calibrateCameraArucoExtended(...)</div><div class="line">        calibrateCameraArucoExtended(corners, ids, counter, board, imageSize, cameraMatrix, distCoeffs[, rvecs[, tvecs[, stdDeviationsIntrinsics[, stdDeviationsExtrinsics[, perViewErrors[, flags[, criteria]</div><div class="line">]]]]]]) -&gt; retval, cameraMatrix, distCoeffs, rvecs, tvecs, stdDeviationsIntrinsics, stdDeviationsExtrinsics, perViewErrors</div><div class="line"></div><div class="line">    calibrateCameraCharuco(...)</div><div class="line">        calibrateCameraCharuco(charucoCorners, charucoIds, board, imageSize, cameraMatrix, distCoeffs[, rvecs[, tvecs[, flags[, criteria]]]]) -&gt; retval, cameraMatrix, distCoeffs, rvecs, tvecs</div><div class="line"></div><div class="line">    calibrateCameraCharucoExtended(...)</div><div class="line">        calibrateCameraCharucoExtended(charucoCorners, charucoIds, board, imageSize, cameraMatrix, distCoeffs[, rvecs[, tvecs[, stdDeviationsIntrinsics[, stdDeviationsExtrinsics[, perViewErrors[, flags[, cr</div><div class="line">iteria]]]]]]]) -&gt; retval, cameraMatrix, distCoeffs, rvecs, tvecs, stdDeviationsIntrinsics, stdDeviationsExtrinsics, perViewErrors</div><div class="line"></div><div class="line">    custom_dictionary(...)</div><div class="line">        custom_dictionary(nMarkers, markerSize) -&gt; retval</div><div class="line"></div><div class="line">    custom_dictionary_from(...)</div><div class="line">        custom_dictionary_from(nMarkers, markerSize, baseDictionary) -&gt; retval</div><div class="line"></div><div class="line">    detectCharucoDiamond(...)</div><div class="line">        detectCharucoDiamond(image, markerCorners, markerIds, squareMarkerLengthRate[, diamondCorners[, diamondIds[, cameraMatrix[, distCoeffs]]]]) -&gt; diamondCorners, diamondIds</div><div class="line"></div><div class="line">    detectMarkers(...)</div><div class="line">        detectMarkers(image, dictionary[, corners[, ids[, parameters[, rejectedImgPoints]]]]) -&gt; corners, ids, rejectedImgPoints</div><div class="line"></div><div class="line">    drawAxis(...)</div><div class="line">        drawAxis(image, cameraMatrix, distCoeffs, rvec, tvec, length) -&gt; image</div><div class="line"></div><div class="line">    drawDetectedCornersCharuco(...)</div><div class="line">        drawDetectedCornersCharuco(image, charucoCorners[, charucoIds[, cornerColor]]) -&gt; image</div><div class="line"></div><div class="line">    drawDetectedDiamonds(...)</div><div class="line">        drawDetectedDiamonds(image, diamondCorners[, diamondIds[, borderColor]]) -&gt; image</div><div class="line"></div><div class="line">    drawDetectedMarkers(...)</div><div class="line">        drawDetectedMarkers(image, corners[, ids[, borderColor]]) -&gt; image</div><div class="line"></div><div class="line">    drawMarker(...)</div><div class="line">        drawMarker(dictionary, id, sidePixels[, img[, borderBits]]) -&gt; img</div><div class="line"></div><div class="line">    drawPlanarBoard(...)</div><div class="line">        drawPlanarBoard(board, outSize[, img[, marginSize[, borderBits]]]) -&gt; img</div><div class="line"></div><div class="line">    estimatePoseBoard(...)</div><div class="line">        estimatePoseBoard(corners, ids, board, cameraMatrix, distCoeffs[, rvec[, tvec[, useExtrinsicGuess]]]) -&gt; retval, rvec, tvec</div><div class="line"></div><div class="line">    estimatePoseCharucoBoard(...)</div><div class="line">        estimatePoseCharucoBoard(charucoCorners, charucoIds, board, cameraMatrix, distCoeffs[, rvec[, tvec[, useExtrinsicGuess]]]) -&gt; retval, rvec, tvec</div><div class="line"></div><div class="line">    estimatePoseSingleMarkers(...)</div><div class="line">        estimatePoseSingleMarkers(corners, markerLength, cameraMatrix, distCoeffs[, rvecs[, tvecs]]) -&gt; rvecs, tvecs</div><div class="line"></div><div class="line">    getPredefinedDictionary(...)</div><div class="line">        getPredefinedDictionary(dict) -&gt; retval</div><div class="line"></div><div class="line">    interpolateCornersCharuco(...)</div><div class="line">        interpolateCornersCharuco(markerCorners, markerIds, image, board[, charucoCorners[, charucoIds[, cameraMatrix[, distCoeffs[, minMarkers]]]]]) -&gt; retval, charucoCorners, charucoIds</div><div class="line"></div><div class="line">    refineDetectedMarkers(...)</div><div class="line">        refineDetectedMarkers(image, board, detectedCorners, detectedIds, rejectedCorners[, cameraMatrix[, distCoeffs[, minRepDistance[, errorCorrectionRate[, checkAllOrders[, recoveredIdxs[, parameters]]]]]]]) -&gt; detectedCorners, detectedIds, rejectedCorners, recoveredIdxs</div><div class="line"></div><div class="line">DATA</div><div class="line">    DICT_4X4_100 = 1</div><div class="line">    DICT_4X4_1000 = 3</div><div class="line">    DICT_4X4_250 = 2</div><div class="line">    DICT_4X4_50 = 0</div><div class="line">    DICT_5X5_100 = 5</div><div class="line">    DICT_5X5_1000 = 7</div><div class="line">    DICT_5X5_250 = 6</div><div class="line">    DICT_5X5_50 = 4</div><div class="line">    DICT_6X6_100 = 9</div><div class="line">    DICT_6X6_1000 = 11</div><div class="line">    DICT_6X6_250 = 10</div><div class="line">    DICT_6X6_50 = 8</div><div class="line">    DICT_7X7_100 = 13</div><div class="line">    DICT_7X7_1000 = 15</div><div class="line">    DICT_7X7_250 = 14</div><div class="line">    DICT_7X7_50 = 12</div><div class="line">    DICT_ARUCO_ORIGINAL = 16</div><div class="line"></div><div class="line">FILE</div><div class="line">    (built-in)</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T19:34:54.000Z"><a href="/2017/03/13/opencv-external-posture-estimation-circle-grid/">2017-03-13</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/13/opencv-external-posture-estimation-circle-grid/">Camera Posture Estimation Using Circle Grid Pattern</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>A widely used asymmetric circle grid pattern can be found in <a href="http://docs.opencv.org/2.4/_downloads/acircles_pattern.png" target="_blank" rel="external">doc of OpenCV 2.4</a>. Same as previous blogs, the camera needs to be calibrated beforehand. For this asymmetric circle grid example, a sequence of images (instead of a video stream) is tested.<p></p>
<p></p><h2>Coding</h2><br>The code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/02_external_camera_posture_estimation/circle_grid.py" target="_blank" rel="external">OpenCV Examples</a>.<p></p>
<p></p><h3>First of all</h3><br>We need to ensure <strong>cv2.so</strong> is under our system path. <strong>cv2.so</strong> is specifically for OpenCV Python.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">sys.path.append(&apos;/usr/local/python/3.5&apos;)</div></pre></td></tr></table></figure><p></p>
<p>Then, we import some packages to be used (<strong>NO ArUco</strong>).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import cv2</div><div class="line">import numpy as np</div></pre></td></tr></table></figure></p>
<p></p><h3>Secondly</h3><br>We now load all camera calibration parameters, including: <strong>cameraMatrix</strong>, <strong>distCoeffs</strong>, etc. For example, your code might look like the following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">calibrationFile = &quot;calibrationFileName.xml&quot;</div><div class="line">calibrationParams = cv2.FileStorage(calibrationFile, cv2.FILE_STORAGE_READ)</div><div class="line">camera_matrix = calibrationParams.getNode(&quot;cameraMatrix&quot;).mat()</div><div class="line">dist_coeffs = calibrationParams.getNode(&quot;distCoeffs&quot;).mat()</div></pre></td></tr></table></figure><p></p>
<p>Since we are testing a calibrated fisheye camera, two extra parameters are to be loaded from the calibration file.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">r = calibrationParams.getNode(&quot;R&quot;).mat()</div><div class="line">new_camera_matrix = calibrationParams.getNode(&quot;newCameraMatrix&quot;).mat()</div></pre></td></tr></table></figure></p>
<p>Afterwards, two mapping matrices are pre-calculated by calling function <a href="http://docs.opencv.org/trunk/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a" target="_blank" rel="external">cv2.fisheye.initUndistortRectifyMap()</a> as (supposing the images to be processed are of 1080P):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">image_size = (1920, 1080)</div><div class="line">map1, map2 = cv2.fisheye.initUndistortRectifyMap(camera_matrix, dist_coeffs, r, new_camera_matrix, image_size, cv2.CV_16SC2)</div></pre></td></tr></table></figure></p>
<p></p><h3>Thirdly</h3><br>The circle pattern is to be loaded.<br><img src="https://raw.githubusercontent.com/LongerVision/OpenCV_Examples/master/markers/pattern_acircles.png" alt="asymmetric_circle_grid" title="asymmetric_circle_grid"><br>Here in our case, this asymmetric circle grid pattern is manually loaded as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"># Original blob coordinates</div><div class="line">objectPoints = np.zeros((44, 3))  # In this asymmetric circle grid, 44 circles are adopted.</div><div class="line">objectPoints[0]  = (0  , 0  , 0)</div><div class="line">objectPoints[1]  = (0  , 72 , 0)</div><div class="line">objectPoints[2]  = (0  , 144, 0)</div><div class="line">objectPoints[3]  = (0  , 216, 0)</div><div class="line">objectPoints[4]  = (36 , 36 , 0)</div><div class="line">objectPoints[5]  = (36 , 108, 0)</div><div class="line">objectPoints[6]  = (36 , 180, 0)</div><div class="line">objectPoints[7]  = (36 , 252, 0)</div><div class="line">objectPoints[8]  = (72 , 0  , 0)</div><div class="line">objectPoints[9]  = (72 , 72 , 0)</div><div class="line">objectPoints[10] = (72 , 144, 0)</div><div class="line">objectPoints[11] = (72 , 216, 0)</div><div class="line">objectPoints[12] = (108, 36,  0)</div><div class="line">objectPoints[13] = (108, 108, 0)</div><div class="line">objectPoints[14] = (108, 180, 0)</div><div class="line">objectPoints[15] = (108, 252, 0)</div><div class="line">objectPoints[16] = (144, 0  , 0)</div><div class="line">objectPoints[17] = (144, 72 , 0)</div><div class="line">objectPoints[18] = (144, 144, 0)</div><div class="line">objectPoints[19] = (144, 216, 0)</div><div class="line">objectPoints[20] = (180, 36 , 0)</div><div class="line">objectPoints[21] = (180, 108, 0)</div><div class="line">objectPoints[22] = (180, 180, 0)</div><div class="line">objectPoints[23] = (180, 252, 0)</div><div class="line">objectPoints[24] = (216, 0  , 0)</div><div class="line">objectPoints[25] = (216, 72 , 0)</div><div class="line">objectPoints[26] = (216, 144, 0)</div><div class="line">objectPoints[27] = (216, 216, 0)</div><div class="line">objectPoints[28] = (252, 36 , 0)</div><div class="line">objectPoints[29] = (252, 108, 0)</div><div class="line">objectPoints[30] = (252, 180, 0)</div><div class="line">objectPoints[31] = (252, 252, 0)</div><div class="line">objectPoints[32] = (288, 0  , 0)</div><div class="line">objectPoints[33] = (288, 72 , 0)</div><div class="line">objectPoints[34] = (288, 144, 0)</div><div class="line">objectPoints[35] = (288, 216, 0)</div><div class="line">objectPoints[36] = (324, 36 , 0)</div><div class="line">objectPoints[37] = (324, 108, 0)</div><div class="line">objectPoints[38] = (324, 180, 0)</div><div class="line">objectPoints[39] = (324, 252, 0)</div><div class="line">objectPoints[40] = (360, 0  , 0)</div><div class="line">objectPoints[41] = (360, 72 , 0)</div><div class="line">objectPoints[42] = (360, 144, 0)</div><div class="line">objectPoints[43] = (360, 216, 0)</div></pre></td></tr></table></figure><p></p>
<p>In our case, the distance between two neighbour circle centres (in the same column) is measured as 72 centimetres. Meanwhile, the axis at the origin is loaded as well, with respective length 300, 200, 100 centimetres.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">axis = np.float32([[360,0,0], [0,240,0], [0,0,-120]]).reshape(-1,3)</div></pre></td></tr></table></figure></p>
<p></p><h3>Fourthly</h3><br>Since we are going to use OpenCV’s SimpleBlobDetector for the blob detection, the SimpleBlobDetector’s parameters are to be created beforehand. The parameter values can be adjusted according to your own testing environments. The iteration <strong>criteria</strong> for the simple blob detection is also created at the same time.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"># Setup SimpleBlobDetector parameters.</div><div class="line">blobParams = cv2.SimpleBlobDetector_Params()</div><div class="line"></div><div class="line"># Change thresholds</div><div class="line">blobParams.minThreshold = 8</div><div class="line">blobParams.maxThreshold = 255</div><div class="line"></div><div class="line"># Filter by Area.</div><div class="line">blobParams.filterByArea = True</div><div class="line">blobParams.minArea = 64     # minArea may be adjusted to suit for your experiment</div><div class="line">blobParams.maxArea = 2500   # maxArea may be adjusted to suit for your experiment</div><div class="line"></div><div class="line"># Filter by Circularity</div><div class="line">blobParams.filterByCircularity = True</div><div class="line">blobParams.minCircularity = 0.1</div><div class="line"></div><div class="line"># Filter by Convexity</div><div class="line">blobParams.filterByConvexity = True</div><div class="line">blobParams.minConvexity = 0.87</div><div class="line"></div><div class="line"># Filter by Inertia</div><div class="line">blobParams.filterByInertia = True</div><div class="line">blobParams.minInertiaRatio = 0.01</div><div class="line"></div><div class="line"># Create a detector with the parameters</div><div class="line">blobDetector = cv2.SimpleBlobDetector_create(blobParams)</div><div class="line"></div><div class="line"># Create the iteration criteria</div><div class="line">criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)</div><div class="line">###################################################################################################</div></pre></td></tr></table></figure><p></p>
<p></p><h3>Finally</h3><br>Estimate camera postures. Here, we are testing a sequence of images, rather than video streams. We first list all file names in sequence.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">imgDir = &quot;imgSequence&quot;  # Specify the image directory</div><div class="line">imgFileNames = [os.path.join(imgDir, fn) for fn in next(os.walk(imgDir))[2]]</div><div class="line">nbOfImgs = len(imgFileNames)</div></pre></td></tr></table></figure><p></p>
<p>Then, we calculate the camera posture frame by frame:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">for i in range(0, nbOfImgs-1):</div><div class="line">    img = cv2.imread(imgFileNames[i], cv2.IMREAD_COLOR)</div><div class="line">    imgRemapped = cv2.remap(img, map1, map2, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT) # for fisheye remapping</div><div class="line">    imgRemapped_gray = cv2.cvtColor(imgRemapped, cv2.COLOR_BGR2GRAY)    # blobDetector.detect() requires gray image</div><div class="line"></div><div class="line">    keypoints = blobDetector.detect(imgRemapped_gray) # Detect blobs.</div><div class="line"></div><div class="line">    # Draw detected blobs as red circles. This helps cv2.findCirclesGrid() .</div><div class="line">    im_with_keypoints = cv2.drawKeypoints(imgRemapped, keypoints, np.array([]), (0,255,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)</div><div class="line">    im_with_keypoints_gray = cv2.cvtColor(im_with_keypoints, cv2.COLOR_BGR2GRAY)</div><div class="line">    ret, corners = cv2.findCirclesGrid(im_with_keypoints, (4,11), None, flags = cv2.CALIB_CB_ASYMMETRIC_GRID)   # Find the circle grid</div><div class="line"></div><div class="line">    if ret == True:</div><div class="line">        corners2 = cv2.cornerSubPix(im_with_keypoints_gray, corners, (11,11), (-1,-1), criteria)    # Refines the corner locations.</div><div class="line"></div><div class="line">        # Draw and display the corners.</div><div class="line">        im_with_keypoints = cv2.drawChessboardCorners(imLeftRemapped, (4,11), corners2, ret)</div><div class="line"></div><div class="line">        # 3D posture</div><div class="line">        if len(corners2) == len(objectPoints):</div><div class="line">            retval, rvec, tvec = cv2.solvePnP(objectPoints, corners2, camera_matrix, dist_coeffs)</div><div class="line"></div><div class="line">        if retval:</div><div class="line">            projectedPoints, jac = cv2.projectPoints(objectPoints, rvec, tvec, camera_matrix, dist_coeffs)  # project 3D points to image plane</div><div class="line">            projectedAxis, jacAsix = cv2.projectPoints(axis, rvec, tvec, camera_matrix, dist_coeffs)    # project axis to image plane</div><div class="line">            for p in projectedPoints:</div><div class="line">                p = np.int32(p).reshape(-1,2)</div><div class="line">                cv2.circle(im_with_keypoints, (p[0][0], p[0][1]), 3, (0,0,255))</div><div class="line">            origin = tuple(corners2[0].ravel())</div><div class="line">            im_with_keypoints = cv2.line(im_with_keypoints, origin, tuple(projectedAxis[0].ravel()), (255,0,0), 2)</div><div class="line">            im_with_keypoints = cv2.line(im_with_keypoints, origin, tuple(projectedAxis[1].ravel()), (0,255,0), 2)</div><div class="line">            im_with_keypoints = cv2.line(im_with_keypoints, origin, tuple(projectedAxis[2].ravel()), (0,0,255), 2)</div><div class="line"></div><div class="line">    cv2.imshow(&quot;circlegrid&quot;, im_with_keypoints) # display</div><div class="line"></div><div class="line">    cv2.waitKey(2)</div></pre></td></tr></table></figure></p>
<p>The drawn axis is just the world coordinators and orientations estimated from the images taken by the testing camera.</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T05:24:56.000Z"><a href="/2017/03/12/opencv-external-posture-estimation-ChArUco-board/">2017-03-12</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/12/opencv-external-posture-estimation-ChArUco-board/">Camera Posture Estimation Using A charuco Board</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>ChAruco is an integrated marker, which combines a chessboard with an aruco marker. The code is also very similar to the code in our previous blog <a href="https://longervision.github.io/2017/03/12/opencv-external-posture-estimation-ArUco-board/">aruco board</a>.<p></p>
<p></p><h2>Coding</h2><br>The code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/02_external_camera_posture_estimation/ChArUco_board.py" target="_blank" rel="external">OpenCV Examples</a>. And the code in the first two subsections are exactly the same as what’s written in our previous blogs. We’ll neglect the first two subsections ever since.<p></p>
<p></p><h3>First of all</h3><br>Exactly the same as in previous blogs.<p></p>
<p></p><h3>Secondly</h3><br>Exactly the same as in previous blogs.<p></p>
<p></p><h3>Thirdly</h3><br>Dictionary <strong>aruco.DICT_6X6_1000</strong> is integrated with a chessboard to construct a grid charuco board. The experimenting board is of size <strong>5*7</strong>, which looks like:<br><img src="https://raw.githubusercontent.com/LongerVision/OpenCV_Examples/master/markers/board_charuco_57.png" alt="charuco.DICT_6X6_1000.board57" title="charuco.DICT_6X6_1000.board57"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aruco_dict = aruco.Dictionary_get( aruco.DICT_6X6_1000 )</div></pre></td></tr></table></figure><p></p>
<p>After having this aruco board marker printed, the edge lengths of this chessboard and aruco marker (displayed in the white cell of the chessboard) are to be measured and stored in two variables <strong>squareLength</strong> and <strong>markerLength</strong>, which are used to create the <strong>5*7</strong> grid board.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">squareLength = 40   # Here, our measurement unit is centimetre.</div><div class="line">markerLength = 30   # Here, our measurement unit is centimetre.</div><div class="line">board = aruco.CharucoBoard_create(5, 7, squareLength, markerLength, aruco_dict)</div></pre></td></tr></table></figure></p>
<p>Meanwhile, create aruco detector with default parameters.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arucoParams = aruco.DetectorParameters_create()</div></pre></td></tr></table></figure></p>
<p></p><h3>Finally</h3><br>Now, let’s test on a video stream, a .mp4 file.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">videoFile = &quot;charuco_board_57.mp4&quot;</div><div class="line">cap = cv2.VideoCapture(videoFile)</div></pre></td></tr></table></figure><p></p>
<p>Then, we calculate the camera posture frame by frame:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">while(True):</div><div class="line">    ret, frame = cap.read() # Capture frame-by-frame</div><div class="line">    if ret == True:</div><div class="line">        frame_remapped = cv2.remap(frame, map1, map2, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)    # for fisheye remapping</div><div class="line">        frame_remapped_gray = cv2.cvtColor(frame_remapped, cv2.COLOR_BGR2GRAY)</div><div class="line"></div><div class="line">        corners, ids, rejectedImgPoints = aruco.detectMarkers(frame_remapped_gray, aruco_dict, parameters=arucoParams)  # First, detect markers</div><div class="line">        aruco.refineDetectedMarkers(frame_remapped_gray, board, corners, ids, rejectedImgPoints)</div><div class="line"></div><div class="line">        if ids != None: # if there is at least one marker detected</div><div class="line">            charucoretval, charucoCorners, charucoIds = aruco.interpolateCornersCharuco(corners, ids, frame_remapped_gray, board)</div><div class="line">            im_with_charuco_board = aruco.drawDetectedCornersCharuco(frame_remapped, charucoCorners, charucoIds, (0,255,0))</div><div class="line">            retval, rvec, tvec = aruco.estimatePoseCharucoBoard(charucoCorners, charucoIds, board, camera_matrix, dist_coeffs)  # posture estimation from a charuco board</div><div class="line">            if retval == True:</div><div class="line">                im_with_charuco_board = aruco.drawAxis(im_with_charuco_board, camera_matrix, dist_coeffs, rvec, tvec, 100)  # axis length 100 can be changed according to your requirement</div><div class="line">        else:</div><div class="line">            im_with_charuco_left = frame_remapped</div><div class="line"></div><div class="line">        cv2.imshow(&quot;charucoboard&quot;, im_with_charuco_board)</div><div class="line"></div><div class="line">        if cv2.waitKey(2) &amp; 0xFF == ord(&apos;q&apos;):</div><div class="line">            break</div><div class="line">    else:</div><div class="line">        break</div></pre></td></tr></table></figure></p>
<p>The drawn axis is just the world coordinators and orientations estimated from the images taken by the testing camera.<br>At the end of the code, we release the video capture handle and destroy all opening windows.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cap.release()   # When everything done, release the capture</div><div class="line">cv2.destroyAllWindows()</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T02:52:18.000Z"><a href="/2017/03/12/opencv-external-posture-estimation-ArUco-board/">2017-03-12</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/12/opencv-external-posture-estimation-ArUco-board/">Camera Posture Estimation Using An aruco Board</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>Today, let’s test on an aruco board, instead of a <a href="https://longervision.github.io/2017/03/10/opencv-external-posture-estimation-ArUco-single-marker/">single marker</a> or a <a href="https://longervision.github.io/2017/03/11/opencv-external-posture-estimation-ArUco-diamond/">diamond marker</a>. Again, you need to make sure your camera has already been calibrated. In the coding section, it’s assumed that you can successfully load the camera calibration parameters.<p></p>
<p></p><h2>Coding</h2><br>The code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/02_external_camera_posture_estimation/ArUco_board.py" target="_blank" rel="external">OpenCV Examples</a>.<p></p>
<p></p><h3>First of all</h3><br>We need to ensure <strong>cv2.so</strong> is under our system path. <strong>cv2.so</strong> is specifically for OpenCV Python.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">sys.path.append(&apos;/usr/local/python/3.5&apos;)</div></pre></td></tr></table></figure><p></p>
<p>Then, we import some packages to be used.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import cv2</div><div class="line">from cv2 import aruco</div><div class="line">import numpy as np</div></pre></td></tr></table></figure></p>
<p></p><h3>Secondly</h3><br>Again, we need to load all camera calibration parameters, including: <strong>cameraMatrix</strong>, <strong>distCoeffs</strong>, etc. :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">calibrationFile = &quot;calibrationFileName.xml&quot;</div><div class="line">calibrationParams = cv2.FileStorage(calibrationFile, cv2.FILE_STORAGE_READ)</div><div class="line">camera_matrix = calibrationParams.getNode(&quot;cameraMatrix&quot;).mat()</div><div class="line">dist_coeffs = calibrationParams.getNode(&quot;distCoeffs&quot;).mat()</div></pre></td></tr></table></figure><p></p>
<p>If you are using a calibrated fisheye camera like us, two extra parameters are to be loaded from the calibration file.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">r = calibrationParams.getNode(&quot;R&quot;).mat()</div><div class="line">new_camera_matrix = calibrationParams.getNode(&quot;newCameraMatrix&quot;).mat()</div></pre></td></tr></table></figure></p>
<p>Afterwards, two mapping matrices are pre-calculated by calling function <a href="http://docs.opencv.org/trunk/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a" target="_blank" rel="external">cv2.fisheye.initUndistortRectifyMap()</a> as (supposing the images to be processed are of 1080P):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">image_size = (1920, 1080)</div><div class="line">map1, map2 = cv2.fisheye.initUndistortRectifyMap(camera_matrix, dist_coeffs, r, new_camera_matrix, image_size, cv2.CV_16SC2)</div></pre></td></tr></table></figure></p>
<p></p><h3>Thirdly</h3><br>In our test, the dictionary <strong>aruco.DICT_6X6_1000</strong> is adopted as the unit pattern to construct a grid board. The board is of size <strong>5*7</strong>, which looks like:<br><img src="https://raw.githubusercontent.com/LongerVision/OpenCV_Examples/master/markers/board_aruco_57.png" alt="aruco.DICT_6X6_1000.board57" title="aruco.DICT_6X6_1000.board57"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aruco_dict = aruco.Dictionary_get( aruco.DICT_6X6_1000 )</div></pre></td></tr></table></figure><p></p>
<p>After having this aruco board marker printed, the edge lengths of this particular aruco marker and the distance between two neighbour markers are to be measured and stored in two variables <strong>markerLength</strong> and <strong>markerSeparation</strong>, which are used to create the <strong>5*7</strong> grid board.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">markerLength = 40   # Here, our measurement unit is centimetre.</div><div class="line">markerSeparation = 8   # Here, our measurement unit is centimetre.</div><div class="line">board = aruco.GridBoard_create(5, 7, markerLength, markerSeparation, aruco_dict)</div></pre></td></tr></table></figure></p>
<p>Meanwhile, create aruco detector with default parameters.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arucoParams = aruco.DetectorParameters_create()</div></pre></td></tr></table></figure></p>
<p></p><h3>Finally</h3><br>Now, let’s test on a video stream, a .mp4 file. We first load the video file and initialize a video capture handle.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">videoFile = &quot;aruco_board_57.mp4&quot;</div><div class="line">cap = cv2.VideoCapture(videoFile)</div></pre></td></tr></table></figure><p></p>
<p>Then, we calculate the camera posture frame by frame:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">while(True):</div><div class="line">    ret, frame = cap.read() # Capture frame-by-frame</div><div class="line">    if ret == True:</div><div class="line">        frame_remapped = cv2.remap(frame, map1, map2, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)     # for fisheye remapping</div><div class="line">        frame_remapped_gray = cv2.cvtColor(frame_remapped, cv2.COLOR_BGR2GRAY)</div><div class="line"></div><div class="line">        corners, ids, rejectedImgPoints = aruco.detectMarkers(frame_remapped_gray, aruco_dict, parameters=arucoParams)  # First, detect markers</div><div class="line">        aruco.refineDetectedMarkers(frame_remapped_gray, board, corners, ids, rejectedImgPoints)</div><div class="line"></div><div class="line">        if ids != None: # if there is at least one marker detected</div><div class="line">            im_with_aruco_board = aruco.drawDetectedMarkers(frame_remapped, corners, ids, (0,255,0))</div><div class="line">            retval, rvec, tvec = aruco.estimatePoseBoard(corners, ids, board, camera_matrix, dist_coeffs)  # posture estimation from a diamond</div><div class="line">            if retval != 0:</div><div class="line">                im_with_aruco_board = aruco.drawAxis(im_with_aruco_board, camera_matrix, dist_coeffs, rvec, tvec, 100)  # axis length 100 can be changed according to your requirement</div><div class="line">        else:</div><div class="line">            im_with_aruco_board = frame_remapped</div><div class="line"></div><div class="line">        cv2.imshow(&quot;arucoboard&quot;, im_with_aruco_board)</div><div class="line"></div><div class="line">        if cv2.waitKey(2) &amp; 0xFF == ord(&apos;q&apos;):</div><div class="line">            break</div><div class="line">    else:</div><div class="line">        break</div></pre></td></tr></table></figure></p>
<p>The drawn axis is just the world coordinators and orientations estimated from the images taken by the testing camera.<br>At the end of the code, we release the video capture handle and destroy all opening windows.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cap.release()   # When everything done, release the capture</div><div class="line">cv2.destroyAllWindows()</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-12T07:45:07.000Z"><a href="/2017/03/11/opencv-external-posture-estimation-ArUco-diamond/">2017-03-11</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/11/opencv-external-posture-estimation-ArUco-diamond/">Camera Posture Estimation Using An aruco Diamond Marker</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>Very similar to our previous post <a href="https://longervision.github.io/2017/03/10/opencv-external-posture-estimation-ArUco-single-marker/">Camera Posture Estimation Using A Single aruco Marker</a>, you need to make sure your camera has already been calibrated. In the coding section, it’s assumed that you can successfully load the camera calibration parameters.<p></p>
<p></p><h2>Coding</h2><br>The code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/02_external_camera_posture_estimation/ArUco_diamond.py" target="_blank" rel="external">OpenCV Examples</a>.<p></p>
<p></p><h3>First of all</h3><br>We need to ensure <strong>cv2.so</strong> is under our system path. <strong>cv2.so</strong> is specifically for OpenCV Python.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">sys.path.append(&apos;/usr/local/python/3.5&apos;)</div></pre></td></tr></table></figure><p></p>
<p>Then, we import some packages to be used.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import cv2</div><div class="line">from cv2 import aruco</div><div class="line">import numpy as np</div></pre></td></tr></table></figure></p>
<p></p><h3>Secondly</h3><br>Again, we need to load all camera calibration parameters, including: <strong>cameraMatrix</strong>, <strong>distCoeffs</strong>, etc. :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">calibrationFile = &quot;calibrationFileName.xml&quot;</div><div class="line">calibrationParams = cv2.FileStorage(calibrationFile, cv2.FILE_STORAGE_READ)</div><div class="line">camera_matrix = calibrationParams.getNode(&quot;cameraMatrix&quot;).mat()</div><div class="line">dist_coeffs = calibrationParams.getNode(&quot;distCoeffs&quot;).mat()</div></pre></td></tr></table></figure><p></p>
<p>If you are using a calibrated fisheye camera like us, two extra parameters are to be loaded from the calibration file.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">r = calibrationParams.getNode(&quot;R&quot;).mat()</div><div class="line">new_camera_matrix = calibrationParams.getNode(&quot;newCameraMatrix&quot;).mat()</div></pre></td></tr></table></figure></p>
<p>Afterwards, two mapping matrices are pre-calculated by calling function <a href="http://docs.opencv.org/trunk/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a" target="_blank" rel="external">cv2.fisheye.initUndistortRectifyMap()</a> as (supposing the images to be processed are of 1080P):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">image_size = (1920, 1080)</div><div class="line">map1, map2 = cv2.fisheye.initUndistortRectifyMap(camera_matrix, dist_coeffs, r, new_camera_matrix, image_size, cv2.CV_16SC2)</div></pre></td></tr></table></figure></p>
<p></p><h3>Thirdly</h3><br>The dictionary <strong>aruco.DICT_6X6_250</strong> is to be loaded. Although current OpenCV provides four groups of <a href="http://docs.opencv.org/trunk/d9/d6a/group__aruco.html" target="_blank" rel="external">aruco</a> patterns, <strong>4X4</strong>, <strong>5X5</strong>, <strong>6X6</strong>, <strong>7X7</strong>, etc., it seems OpenCV Python does NOT provide a function named <a href="http://docs.opencv.org/trunk/d9/d6a/group__aruco.html#gaf71fb897d5f03f7424c0c84715aa6228" target="_blank" rel="external">drawCharucoDiamond()</a>. Therefore, we have to refer to the C++ tutorial <a href="http://docs.opencv.org/trunk/d5/d07/tutorial_charuco_diamond_detection.html" target="_blank" rel="external">Detection of Diamond Markers</a>. And, we directly use this particular diamond marker in the tutorial:<br><img src="http://docs.opencv.org/trunk/diamondmarker.png" alt="aruco.DICT_6X6_250.diamond" title="aruco.DICT_6X6_250.diamond"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aruco_dict = aruco.Dictionary_get( aruco.DICT_6X6_250 )</div></pre></td></tr></table></figure><p></p>
<p>After having this aruco diamond marker printed, the edge lengths of this particular diamond marker are to be measured and stored in two variables <strong>squareLength</strong> and <strong>markerLength</strong>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">squareLength = 40   # Here, our measurement unit is centimetre.</div><div class="line">markerLength = 25   # Here, our measurement unit is centimetre.</div></pre></td></tr></table></figure></p>
<p>Meanwhile, create aruco detector with default parameters.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arucoParams = aruco.DetectorParameters_create()</div></pre></td></tr></table></figure></p>
<p></p><h3>Finally</h3><br>This time, let’s test on a video stream, a .mp4 file. We first load the video file and initialize a video capture handle.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">videoFile = &quot;aruco_diamond.mp4&quot;</div><div class="line">cap = cv2.VideoCapture(videoFile)</div></pre></td></tr></table></figure><p></p>
<p>Then, we calculate the camera posture frame by frame:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">while(True):</div><div class="line">    ret, frame = cap.read() # Capture frame-by-frame</div><div class="line">    if ret == True:</div><div class="line">        frame_remapped = cv2.remap(frame, map1, map2, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)     # for fisheye remapping</div><div class="line">        frame_remapped_gray = cv2.cvtColor(frame_remapped, cv2.COLOR_BGR2GRAY)</div><div class="line"></div><div class="line">        corners, ids, rejectedImgPoints = aruco.detectMarkers(frame_remapped_gray, aruco_dict, parameters=arucoParams)  # First, detect markers</div><div class="line"></div><div class="line">        if ids != None: # if there is at least one marker detected</div><div class="line">            diamondCorners, diamondIds = aruco.detectCharucoDiamond(frame_remapped_gray, corners, ids, squareLength/markerLength)   # Second, detect diamond markers</div><div class="line">            if len(diamondCorners) &gt;= 1:    # if there is at least one diamond detected</div><div class="line">                im_with_diamond = aruco.drawDetectedDiamonds(frame_remapped, diamondCorners, diamondIds, (0,255,0))</div><div class="line">                rvec, tvec = aruco.estimatePoseSingleMarkers(diamondCorners, squareLength, camera_matrix, dist_coeffs)  # posture estimation from a diamond</div><div class="line">                im_with_diamond = aruco.drawAxis(im_with_diamond, camera_matrix, dist_coeffs, rvec, tvec, 100)  # axis length 100 can be changed according to your requirement</div><div class="line">        else:</div><div class="line">            im_with_diamond = frame_remapped</div><div class="line"></div><div class="line">        cv2.imshow(&quot;diamondLeft&quot;, im_with_diamond)   # display</div><div class="line"></div><div class="line">        if cv2.waitKey(2) &amp; 0xFF == ord(&apos;q&apos;):   # press &apos;q&apos; to quit</div><div class="line">            break</div><div class="line">    else:</div><div class="line">        break</div></pre></td></tr></table></figure></p>
<p>The drawn axis is just the world coordinators and orientations estimated from the images taken by the testing camera.<br>At the end of the code, we release the video capture handle and destroy all opening windows.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cap.release()   # When everything done, release the capture</div><div class="line">cv2.destroyAllWindows()</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-11T07:11:07.000Z"><a href="/2017/03/10/opencv-external-posture-estimation-ArUco-single-marker/">2017-03-10</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/10/opencv-external-posture-estimation-ArUco-single-marker/">Camera Posture Estimation Using A Single aruco Marker</a></h1>
  

    </header>
    <div class="entry">
      
        <p></p><h2>Preparation</h2><br>Before start coding, you need to ensure your camera has already been calibrated. (Camera calibration is covered in our blog as well.) In the coding section, it’s assumed that you can successfully load the camera calibration parameters.<p></p>
<p></p><h2>Coding</h2><br>The code can be found at <a href="https://github.com/LongerVision/OpenCV_Examples/blob/master/02_external_camera_posture_estimation/ArUco_single_marker.py" target="_blank" rel="external">OpenCV Examples</a>.<p></p>
<p></p><h3>First of all</h3><br>We need to ensure <strong>cv2.so</strong> is under our system path. <strong>cv2.so</strong> is specifically for OpenCV Python.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">sys.path.append(&apos;/usr/local/python/3.5&apos;)</div></pre></td></tr></table></figure><p></p>
<p>Then, we import some packages to be used.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">import cv2</div><div class="line">from cv2 import aruco</div><div class="line">import numpy as np</div></pre></td></tr></table></figure></p>
<p></p><h3>Secondly</h3><br>We now load all camera calibration parameters, including: <strong>cameraMatrix</strong>, <strong>distCoeffs</strong>, etc. For example, your code might look like the following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">calibrationFile = &quot;calibrationFileName.xml&quot;</div><div class="line">calibrationParams = cv2.FileStorage(calibrationFile, cv2.FILE_STORAGE_READ)</div><div class="line">camera_matrix = calibrationParams.getNode(&quot;cameraMatrix&quot;).mat()</div><div class="line">dist_coeffs = calibrationParams.getNode(&quot;distCoeffs&quot;).mat()</div></pre></td></tr></table></figure><p></p>
<p>Since we are testing a calibrated fisheye camera, two extra parameters are to be loaded from the calibration file.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">r = calibrationParams.getNode(&quot;R&quot;).mat()</div><div class="line">new_camera_matrix = calibrationParams.getNode(&quot;newCameraMatrix&quot;).mat()</div></pre></td></tr></table></figure></p>
<p>Afterwards, two mapping matrices are pre-calculated by calling function <a href="http://docs.opencv.org/trunk/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a" target="_blank" rel="external">cv2.fisheye.initUndistortRectifyMap()</a> as (supposing the images to be processed are of 1080P):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">image_size = (1920, 1080)</div><div class="line">map1, map2 = cv2.fisheye.initUndistortRectifyMap(camera_matrix, dist_coeffs, r, new_camera_matrix, image_size, cv2.CV_16SC2)</div></pre></td></tr></table></figure></p>
<p></p><h3>Thirdly</h3><br>A dictionary is to be loaded. Current OpenCV provides four groups of <a href="http://docs.opencv.org/trunk/d9/d6a/group__aruco.html" target="_blank" rel="external">aruco</a> patterns, <strong>4X4</strong>, <strong>5X5</strong>, <strong>6X6</strong>, <strong>7X7</strong>, etc. Here, <strong>aruco.DICT_6X6_1000</strong> is randomly selected as our example, which looks like:<br><img src="https://raw.githubusercontent.com/LongerVision/OpenCV_Examples/master/markers/marker_66.jpg" alt="aruco.DICT_6X6_1000" title="aruco.DICT_6X6_1000"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aruco_dict = aruco.Dictionary_get( aruco.DICT_6X6_1000 )</div></pre></td></tr></table></figure><p></p>
<p>After having this aruco square marker printed, the edge length of this particular marker is to be measured and stored in a variable <strong>markerLength</strong>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">markerLength = 20 # Here, our measurement unit is centimetre.</div></pre></td></tr></table></figure></p>
<p>Meanwhile, create aruco detector with default parameters.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arucoParams = aruco.DetectorParameters_create()</div></pre></td></tr></table></figure></p>
<p></p><h3>Finally</h3><br>Estimate camera postures. Here, we are testing a sequence of images, rather than video streams. We first list all file names in sequence.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">imgDir = &quot;imgSequence&quot;  # Specify the image directory</div><div class="line">imgFileNames = [os.path.join(imgDir, fn) for fn in next(os.walk(imgDir))[2]]</div><div class="line">nbOfImgs = len(imgFileNames)</div></pre></td></tr></table></figure><p></p>
<p>Then, we calculate the camera posture frame by frame:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">for i in range(0, nbOfImgs):</div><div class="line">    img = cv2.imread(imgFileNames[i], cv2.IMREAD_COLOR)</div><div class="line">    imgRemapped = cv2.remap(img, map1, map2, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT) # for fisheye remapping</div><div class="line">    imgRemapped_gray = cv2.cvtColor(imgRemapped, cv2.COLOR_BGR2GRAY)    # aruco.etectMarkers() requires gray image</div><div class="line">    corners, ids, rejectedImgPoints = aruco.detectMarkers(imgRemapped_gray, aruco_dict, parameters=arucoParams) # Detect aruco</div><div class="line">    if ids != None: # if aruco marker detected</div><div class="line">        rvec, tvec = aruco.estimatePoseSingleMarkers(corners, markerLength, camera_matrix, dist_coeffs) # For a single marker</div><div class="line">        imgWithAruco = aruco.drawDetectedMarkers(imgRemapped, corners, ids, (0,255,0))</div><div class="line">        imgWithAruco = aruco.drawAxis(imgWithAruco, camera_matrix, dist_coeffs, rvec, tvec, 100)    # axis length 100 can be changed according to your requirement</div><div class="line">    else:   # if aruco marker is NOT detected</div><div class="line">        imgWithAruco = imgRemapped  # assign imRemapped_color to imgWithAruco directly</div><div class="line"></div><div class="line">    cv2.imshow(&quot;aruco&quot;, imgWithAruco)   # display</div><div class="line"></div><div class="line">    if cv2.waitKey(2) &amp; 0xFF == ord(&apos;q&apos;):   # if &apos;q&apos; is pressed, quit.</div><div class="line">        break</div></pre></td></tr></table></figure></p>
<p>The drawn axis is just the world coordinators and orientations estimated from the images taken by the testing camera.</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-10T21:11:07.000Z"><a href="/2017/03/10/hi-nobody/">2017-03-10</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/10/hi-nobody/">Hi, Everyone...</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Hi, everyone. This is Nobody from <a href="http://www.longervision.ca" target="_blank" rel="external">Longer Vision Technology</a>. I come back to life, at least, half life. And finally, I decided to write something, either useful, or useless. Hope my blogs will be able to help some of the pure researchers, as well as the students, in the field of Computer Vision &amp; Machine Vision. By the way, our products will be put on sale soon. Keep an eye on our blogs please. Thank you…</p>
<!--
## Quick Start

### Create a new post

<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a>–&gt;</p>
-->
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:longervision.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/ArUco-Marker-Pattern-OpenCV-Python/">ArUco, Marker, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Calibration-Chessboard-Marker-Pattern-OpenCV-Python/">Camera Calibration, Chessboard, Marker, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Calibration-Circle-Grid-Marker-Pattern-OpenCV-Python/">Camera Calibration, Circle Grid, Marker, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Posture-Estimation-Circle-Grid-Marker-Pattern-OpenCV-Python/">Camera Posture Estimation, Circle Grid, Marker, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Posture-Estimation-aruco-Board-Pattern-OpenCV-Python/">Camera Posture Estimation, aruco Board, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Posture-Estimation-aruco-Diamond-Marker-Pattern-OpenCV-Python/">Camera Posture Estimation, aruco, Diamond Marker, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Posture-Estimation-aruco-Marker-Pattern-OpenCV-Python/">Camera Posture Estimation, aruco, Marker, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Camera-Posture-Estimation-charuco-Board-Pattern-OpenCV-Python/">Camera Posture Estimation, charuco Board, Pattern, OpenCV, Python</a><small>1</small></li>
  
    <li><a href="/tags/Longer-Vision-Technology-Computer-Vision-Machine-Vision/">Longer Vision Technology, Computer Vision, Machine Vision</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Nobody
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
