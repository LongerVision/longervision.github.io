<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"longervision.ca","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.23.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Peace. Jesus. I&#39;d recommend you The Bible Studio. Today, I&#39;d love to have some fun of TorchServe. 1. TorchServe Getting Started Please just follow TorchServe Getting Started, with trivial modification">
<meta property="og:type" content="article">
<meta property="og:title" content="TorchServe">
<meta property="og:url" content="http://longervision.ca/2024/10/06/AI/DeepLearning/TorchServe/index.html">
<meta property="og:site_name" content="Longer Vision Technology">
<meta property="og:description" content="Peace. Jesus. I&#39;d recommend you The Bible Studio. Today, I&#39;d love to have some fun of TorchServe. 1. TorchServe Getting Started Please just follow TorchServe Getting Started, with trivial modification">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-10-07T06:55:34.000Z">
<meta property="article:modified_time" content="2025-06-04T06:08:38.377Z">
<meta property="article:author" content="Nobody">
<meta property="article:tag" content="CNN, PyTorch, TorchServe, Deep Learning, Machine Learning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://longervision.ca/2024/10/06/AI/DeepLearning/TorchServe/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://longervision.ca/2024/10/06/AI/DeepLearning/TorchServe/","path":"2024/10/06/AI/DeepLearning/TorchServe/","title":"TorchServe"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TorchServe | Longer Vision Technology</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Longer Vision Technology</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Github Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#torchserve-getting-started"><span class="nav-number">1.</span> <span class="nav-text">1. TorchServe Getting
Started</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#clone-torchserve"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Clone TorchServe</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#store-a-model"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Store a Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#start-torchserve"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Start TorchServe</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#config.properties"><span class="nav-number">1.3.1.</span> <span class="nav-text">1.3.1 config.properties</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#start-torchserve-to-serve-the-model"><span class="nav-number">1.3.2.</span> <span class="nav-text">1.3.2 Start TorchServe to serve the
model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#key_file.json"><span class="nav-number">1.3.3.</span> <span class="nav-text">1.3.3 key_file.json</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#is-torchserve-service-running"><span class="nav-number">1.3.4.</span> <span class="nav-text">1.3.4 Is TorchServe Service
Running?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#get-predictions"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 Get Predictions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#using-rest-apis"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.4.1 Using REST APIs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#using-grpc-apis-through-python-client"><span class="nav-number">1.4.2.</span> <span class="nav-text">1.4.2 Using gRPC APIs through Python Client</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#install-grpc-python-dependencies"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">1.4.2.1 Install gRPC Python dependencies</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#generate-inference-client-using-proto-files"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">1.4.2.2 Generate
inference client using proto files</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Nobody</p>
  <div class="site-description" itemprop="description">Longer Vision Technology Github Blog</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">154</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">144</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://longervision.ca/2024/10/06/AI/DeepLearning/TorchServe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Nobody">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Longer Vision Technology">
      <meta itemprop="description" content="Longer Vision Technology Github Blog">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TorchServe | Longer Vision Technology">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TorchServe
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-06 23:55:34" itemprop="dateCreated datePublished" datetime="2024-10-06T23:55:34-07:00">2024-10-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-06-03 23:08:38" itemprop="dateModified" datetime="2025-06-03T23:08:38-07:00">2025-06-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Peace. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Jesus">Jesus</a>. I'd
recommend you <a target="_blank" rel="noopener" href="https://www.thebible.studio/">The Bible
Studio</a>.</p>
<p>Today, I'd love to have some fun of <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/index.html">TorchServe</a>.</p>
<h1 id="torchserve-getting-started">1. <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/getting_started.html">TorchServe Getting
Started</a></h1>
<p>Please just follow <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/getting_started.html">TorchServe Getting
Started</a>, with trivial modifications. In my demonstration, I stick to
working under directory <code>/opt/servers/torchserve</code>.</p>
<h2 id="clone-torchserve">1.1 Clone <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/index.html">TorchServe</a></h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve git clone https://github.com/pytorch/serve.git        </span><br><span class="line">Cloning into &#x27;serve&#x27;...</span><br><span class="line">remote: Enumerating objects: 60508, done.</span><br><span class="line">remote: Counting objects: 100% (25/25), done.</span><br><span class="line">remote: Compressing objects: 100% (25/25), done.</span><br><span class="line">remote: Total 60508 (delta 6), reused 3 (delta 0), pack-reused 60483 (from 1)</span><br><span class="line">Receiving objects: 100% (60508/60508), 99.06 MiB | 23.01 MiB/s, done.</span><br><span class="line">Resolving deltas: 100% (37656/37656), done.</span><br></pre></td></tr></table></figure>
<h2 id="store-a-model">1.2 Store a Model</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve mkdir model_store</span><br><span class="line">➜  torchserve wget https://download.pytorch.org/models/densenet161-8d451a50.pth</span><br><span class="line">--2024-10-07 00:35:54--  https://download.pytorch.org/models/densenet161-8d451a50.pth</span><br><span class="line">Resolving download.pytorch.org (download.pytorch.org)... 2600:9000:26ce:7a00:d:607e:4540:93a1, 2600:9000:26ce:6e00:d:607e:4540:93a1, 2600:9000:26ce:3400:d:607e:4540:93a1, ...</span><br><span class="line">Connecting to download.pytorch.org (download.pytorch.org)|2600:9000:26ce:7a00:d:607e:4540:93a1|:443... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 115730790 (110M) [application/x-www-form-urlencoded]</span><br><span class="line">Saving to: ‘densenet161-8d451a50.pth’</span><br><span class="line"></span><br><span class="line">densenet161-8d451a50.pth                                       100%[==================================================================================================================================================&gt;] 110.37M   109MB/s    in 1.0s    </span><br><span class="line"></span><br><span class="line">2024-10-07 00:35:55 (109 MB/s) - ‘densenet161-8d451a50.pth’ saved [115730790/115730790]</span><br><span class="line">➜  torchserve torch-model-archiver --model-name densenet161 --version 1.0 --model-file ./serve/examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --export-path model_store --extra-files ./serve/examples/image_classifier/index_to_name.json --handler image_classifier</span><br><span class="line">➜  torchserve ll model_store</span><br><span class="line">total 106M</span><br><span class="line">4.0K drwxrwxr-x 2 lvision lvision 4.0K Oct  7 00:36 ./</span><br><span class="line">4.0K drwxrwxr-x 5 lvision lvision 4.0K Oct  7 00:35 ../</span><br><span class="line">106M -rw-rw-r-- 1 lvision lvision 106M Oct  7 00:36 densenet161.mar</span><br></pre></td></tr></table></figure>
<h2 id="start-torchserve">1.3 Start <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/index.html">TorchServe</a></h2>
<h3 id="config.properties">1.3.1 <code>config.properties</code></h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve cat config.properties </span><br><span class="line">inference_address=http://127.0.0.1:8080</span><br><span class="line">management_address=http://127.0.0.1:8081</span><br><span class="line">metrics_address=http://127.0.0.1:8082</span><br><span class="line">enable_token_auth=true</span><br><span class="line">ts.key.file=/opt/servers/torchserve/key_file.json</span><br><span class="line">log_location=/opt/servers/torchserve/logs</span><br><span class="line">metrics_location=/opt/servers/torchserve/logs</span><br><span class="line">access_log_location=/opt/servers/torchserve/logs</span><br></pre></td></tr></table></figure>
<h3 id="start-torchserve-to-serve-the-model">1.3.2 Start <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/index.html">TorchServe</a> to serve the
model</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve torchserve --start --ncs --model-store model_store --models densenet161.mar</span><br><span class="line">➜  torchserve WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.</span><br><span class="line">2024-10-07T00:38:23,323 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program &quot;xpu-smi&quot;: error=2, No such file or directory</span><br><span class="line">2024-10-07T00:38:23,326 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties</span><br><span class="line">2024-10-07T00:38:23,361 [INFO ] main org.pytorch.serve.util.TokenAuthorization - </span><br><span class="line">######</span><br><span class="line">TorchServe now enforces token authorization by default.</span><br><span class="line">This requires the correct token to be provided when calling an API.</span><br><span class="line">Key file located at /opt/servers/torchserve/key_file.json</span><br><span class="line">Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md </span><br><span class="line">######</span><br><span class="line"></span><br><span class="line">2024-10-07T00:38:23,361 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...</span><br><span class="line">2024-10-07T00:38:23,399 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/lvision/.local/lib/python3.12/site-packages/ts/configs/metrics.yaml</span><br><span class="line">2024-10-07T00:38:23,484 [INFO ] main org.pytorch.serve.ModelServer - </span><br><span class="line">Torchserve version: 0.12.0</span><br><span class="line">TS Home: /home/lvision/.local/lib/python3.12/site-packages</span><br><span class="line">Current directory: /opt/servers/torchserve</span><br><span class="line">Temp directory: /tmp</span><br><span class="line">Metrics config path: /home/lvision/.local/lib/python3.12/site-packages/ts/configs/metrics.yaml</span><br><span class="line">Number of GPUs: 1</span><br><span class="line">Number of CPUs: 48</span><br><span class="line">Max heap size: 30208 M</span><br><span class="line">Python executable: /usr/bin/python3</span><br><span class="line">Config file: config.properties</span><br><span class="line">Inference address: http://127.0.0.1:8080</span><br><span class="line">Management address: http://127.0.0.1:8081</span><br><span class="line">Metrics address: http://127.0.0.1:8082</span><br><span class="line">Model Store: /opt/servers/torchserve/model_store</span><br><span class="line">Initial Models: densenet161.mar</span><br><span class="line">Log dir: /opt/servers/torchserve/logs</span><br><span class="line">Metrics dir: /opt/servers/torchserve/logs</span><br><span class="line">Netty threads: 0</span><br><span class="line">Netty client threads: 0</span><br><span class="line">Default workers per model: 1</span><br><span class="line">Blacklist Regex: N/A</span><br><span class="line">Maximum Response Size: 6553500</span><br><span class="line">Maximum Request Size: 6553500</span><br><span class="line">Limit Maximum Image Pixels: true</span><br><span class="line">Prefer direct buffer: false</span><br><span class="line">Allowed Urls: [file://.*|http(s)?://.*]</span><br><span class="line">Custom python dependency for model allowed: false</span><br><span class="line">Enable metrics API: true</span><br><span class="line">Metrics mode: LOG</span><br><span class="line">Disable system metrics: false</span><br><span class="line">Workflow Store: /opt/servers/torchserve/model_store</span><br><span class="line">CPP log config: N/A</span><br><span class="line">Model config: N/A</span><br><span class="line">System metrics command: default</span><br><span class="line">Model API enabled: false</span><br><span class="line">2024-10-07T00:38:23,491 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...</span><br><span class="line">2024-10-07T00:38:23,492 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: densenet161.mar</span><br><span class="line">2024-10-07T00:38:24,521 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model densenet161</span><br><span class="line">2024-10-07T00:38:24,521 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model densenet161</span><br><span class="line">2024-10-07T00:38:24,521 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model densenet161 loaded.</span><br><span class="line">2024-10-07T00:38:24,521 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: densenet161, count: 1</span><br><span class="line">2024-10-07T00:38:24,526 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.</span><br><span class="line">2024-10-07T00:38:24,527 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/lvision/.local/lib/python3.12/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/lvision/.local/lib/python3.12/site-packages/ts/configs/metrics.yaml]</span><br><span class="line">2024-10-07T00:38:24,568 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080</span><br><span class="line">2024-10-07T00:38:24,568 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.</span><br><span class="line">2024-10-07T00:38:24,569 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081</span><br><span class="line">2024-10-07T00:38:24,570 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.</span><br><span class="line">2024-10-07T00:38:24,570 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082</span><br><span class="line">Model server started.</span><br><span class="line">2024-10-07T00:38:24,720 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.</span><br><span class="line">2024-10-07T00:38:25,152 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:20.0|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,153 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:1412.4598426818848|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,153 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:215.69704055786133|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,153 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.2|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,153 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:2.5472005208333335|#Level:Host,DeviceId:0|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,153 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:626.0|#Level:Host,DeviceId:0|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,154 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,154 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:248234.90625|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,154 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:6890.875|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,154 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:3.6|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286705</span><br><span class="line">2024-10-07T00:38:25,632 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=62344</span><br><span class="line">2024-10-07T00:38:25,636 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000</span><br><span class="line">2024-10-07T00:38:25,637 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Successfully loaded /home/lvision/.local/lib/python3.12/site-packages/ts/configs/metrics.yaml.</span><br><span class="line">2024-10-07T00:38:25,637 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - [PID]62344</span><br><span class="line">2024-10-07T00:38:25,637 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch worker started.</span><br><span class="line">2024-10-07T00:38:25,637 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Python runtime: 3.12.3</span><br><span class="line">2024-10-07T00:38:25,638 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change null -&gt; WORKER_STARTED</span><br><span class="line">2024-10-07T00:38:25,641 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000</span><br><span class="line">2024-10-07T00:38:25,646 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.</span><br><span class="line">2024-10-07T00:38:25,648 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1728286705648</span><br><span class="line">2024-10-07T00:38:25,649 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1728286705649</span><br><span class="line">2024-10-07T00:38:25,674 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - model_name: densenet161, batchSize: 1</span><br><span class="line">2024-10-07T00:38:26,881 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Enabled tensor cores</span><br><span class="line">2024-10-07T00:38:26,882 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - OpenVINO is not enabled</span><br><span class="line">2024-10-07T00:38:26,882 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - proceeding without onnxruntime</span><br><span class="line">2024-10-07T00:38:26,882 [INFO ] W-9000-densenet161_1.0-stdout MODEL_LOG - Torch TensorRT not enabled</span><br><span class="line">2024-10-07T00:38:27,263 [WARN ] W-9000-densenet161_1.0-stderr MODEL_LOG - /home/lvision/.local/lib/python3.12/site-packages/ts/torch_handler/base_handler.py:355: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#x27;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.</span><br><span class="line">2024-10-07T00:38:27,264 [WARN ] W-9000-densenet161_1.0-stderr MODEL_LOG -   state_dict = torch.load(model_pt_path, map_location=map_location)</span><br><span class="line">2024-10-07T00:38:27,564 [INFO ] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1915</span><br><span class="line">2024-10-07T00:38:27,564 [DEBUG] W-9000-densenet161_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-densenet161_1.0 State change WORKER_STARTED -&gt; WORKER_MODEL_LOADED</span><br><span class="line">2024-10-07T00:38:27,565 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:3041.0|#WorkerName:W-9000-densenet161_1.0,Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286707</span><br><span class="line">2024-10-07T00:38:27,565 [INFO ] W-9000-densenet161_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:lvision-MS-7C60,timestamp:1728286707</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="key_file.json">1.3.3 <code>key_file.json</code></h3>
<p>By running the above command, a <code>key_file.json</code> file is
generated under the current working directory (Please refer to <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/token_authorization_api.html">TorchServe
token authorization API</a>):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve ll key_file.json</span><br><span class="line">4.0K -rw------- 1 lvision lvision 243 Oct  7 00:38 key_file.json</span><br><span class="line">➜  torchserve cat key_file.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;management&quot;: &#123;</span><br><span class="line">    &quot;key&quot;: &quot;c_7-MgUE&quot;,</span><br><span class="line">    &quot;expiration time&quot;: &quot;2024-10-07T08:38:23.343462778Z&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;inference&quot;: &#123;</span><br><span class="line">    &quot;key&quot;: &quot;IMc5oeRf&quot;,</span><br><span class="line">    &quot;expiration time&quot;: &quot;2024-10-07T08:38:23.343456097Z&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;API&quot;: &#123;</span><br><span class="line">    &quot;key&quot;: &quot;_tFv4L56&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;% </span><br></pre></td></tr></table></figure>
<h3 id="is-torchserve-service-running">1.3.4 Is <a
target="_blank" rel="noopener" href="https://pytorch.org/serve/index.html">TorchServe</a> Service
Running?</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ curl -H &quot;Authorization: Bearer c_7-MgUE&quot; http://127.0.0.1:8081/models</span><br><span class="line">&#123;</span><br><span class="line">  &quot;models&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;modelName&quot;: &quot;densenet161&quot;,</span><br><span class="line">      &quot;modelUrl&quot;: &quot;densenet161.mar&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="get-predictions">1.4 Get Predictions</h2>
<h3 id="using-rest-apis">1.4.1 Using <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/REST">REST</a> APIs</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100  7341  100  7341    0     0  28980      0 --:--:-- --:--:-- --:--:-- 29015</span><br></pre></td></tr></table></figure>
<p>Let's take a look:</p>
<p><a
target="_blank" rel="noopener" href="https://raw.githubusercontent.com/LongerVision/Resource/refs/heads/main/AI/PyTorch/kitten_small.jpg"><img
src="https://raw.githubusercontent.com/LongerVision/Resource/refs/heads/main/AI/PyTorch/kitten_small.jpg"
alt="kitten" /></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve curl -H &quot;Authorization: Bearer IMc5oeRf&quot; http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tabby&quot;: 0.47793325781822205,</span><br><span class="line">  &quot;lynx&quot;: 0.20019005239009857,</span><br><span class="line">  &quot;tiger_cat&quot;: 0.16827784478664398,</span><br><span class="line">  &quot;tiger&quot;: 0.062009651213884354,</span><br><span class="line">  &quot;Egyptian_cat&quot;: 0.05115227773785591</span><br><span class="line">&#125;%</span><br></pre></td></tr></table></figure>
<h3 id="using-grpc-apis-through-python-client">1.4.2 Using <a
target="_blank" rel="noopener" href="https://grpc.io/">gRPC</a> APIs through Python Client</h3>
<h4 id="install-grpc-python-dependencies">1.4.2.1 Install <a
target="_blank" rel="noopener" href="https://grpc.io/">gRPC</a> Python dependencies</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U grpcio protobuf grpcio-tools googleapis-common-protos</span><br></pre></td></tr></table></figure>
<h4 id="generate-inference-client-using-proto-files">1.4.2.2 Generate
inference client using proto files</h4>
<p><strong><span style="color:red">Must under folder
<code>serve</code></span></strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  torchserve cd serve</span><br></pre></td></tr></table></figure>
<p>Then,</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  serve git:(master) python -m grpc_tools.protoc --proto_path=frontend/server/src/main/resources/proto/ --python_out=ts_scripts --grpc_python_out=ts_scripts frontend/server/src/main/resources/proto/inference.proto frontend/server/src/main/resources/proto/management.proto</span><br><span class="line">google/rpc/status.proto: File not found.</span><br><span class="line">inference.proto:6:1: Import &quot;google/rpc/status.proto&quot; was not found or had errors.</span><br><span class="line">inference.proto:32:14: &quot;google.rpc.Status&quot; is not defined.</span><br></pre></td></tr></table></figure>
<strong><span style="color:red">Why????</span></strong>
<div style="text-align: center; font-size: 3em;">
<pre><code>😞😢😭</code></pre>
</div>
<p><strong><span style="color:red">Solution:</span></strong>:</p>
<ul>
<li><strong><span style="color:red">Step 1: Clone <a
target="_blank" rel="noopener" href="https://github.com/googleapis/googleapis">Google
APIs</a></span></strong>:</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  serve git:(master) git clone https://github.com/googleapis/googleapis.git</span><br><span class="line"></span><br><span class="line">Cloning into &#x27;googleapis&#x27;...</span><br><span class="line">remote: Enumerating objects: 233669, done.</span><br><span class="line">remote: Counting objects: 100% (13457/13457), done.</span><br><span class="line">remote: Compressing objects: 100% (410/410), done.</span><br><span class="line">remote: Total 233669 (delta 13122), reused 13077 (delta 13042), pack-reused 220212 (from 1)</span><br><span class="line">Receiving objects: 100% (233669/233669), 205.13 MiB | 21.65 MiB/s, done.</span><br><span class="line">Resolving deltas: 100% (196982/196982), done.</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><span style="color:red">Step 2: Generate inference client
using <a target="_blank" rel="noopener" href="https://github.com/googleapis/googleapis">Google
APIs</a>'s necessary <code>.proto</code> files</span></strong>:</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  serve git:(master) ✗ python -m grpc_tools.protoc \</span><br><span class="line">  --proto_path=frontend/server/src/main/resources/proto/ \</span><br><span class="line">  --proto_path=googleapis/ \</span><br><span class="line">  --python_out=ts_scripts \</span><br><span class="line">  --grpc_python_out=ts_scripts \</span><br><span class="line">  frontend/server/src/main/resources/proto/inference.proto \</span><br><span class="line">  frontend/server/src/main/resources/proto/management.proto</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><span style="color:red">Step 3: Modify
<code>ts_scripts/torchserve_grpc_client.py</code> as
follows</span></strong>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> grpc</span><br><span class="line"><span class="keyword">import</span> inference_pb2</span><br><span class="line"><span class="keyword">import</span> inference_pb2_grpc</span><br><span class="line"><span class="keyword">import</span> management_pb2</span><br><span class="line"><span class="keyword">import</span> management_pb2_grpc</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to get an inference stub for making gRPC calls to the inference service.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_inference_stub</span>():</span><br><span class="line">    channel = grpc.insecure_channel(<span class="string">&quot;localhost:7070&quot;</span>)</span><br><span class="line">    stub = inference_pb2_grpc.InferenceAPIsServiceStub(channel)</span><br><span class="line">    <span class="keyword">return</span> stub</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to get a management stub for making gRPC calls to the model management service.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_management_stub</span>():</span><br><span class="line">    channel = grpc.insecure_channel(<span class="string">&quot;localhost:7071&quot;</span>)</span><br><span class="line">    stub = management_pb2_grpc.ManagementAPIsServiceStub(channel)</span><br><span class="line">    <span class="keyword">return</span> stub</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform a single inference call.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer</span>(<span class="params">stub, model_name, model_input, metadata</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(model_input, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.read()</span><br><span class="line"></span><br><span class="line">    input_data = &#123;<span class="string">&quot;data&quot;</span>: data&#125;</span><br><span class="line">    response = stub.Predictions(</span><br><span class="line">        inference_pb2.PredictionsRequest(model_name=model_name, <span class="built_in">input</span>=input_data),</span><br><span class="line">        metadata=metadata,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        prediction = response.prediction.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(prediction)</span><br><span class="line">    <span class="keyword">except</span> grpc.RpcError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;gRPC error: <span class="subst">&#123;e.details()&#125;</span>&quot;</span>)</span><br><span class="line">        exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform streaming inference.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer_stream</span>(<span class="params">stub, model_name, model_input, metadata</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(model_input, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.read()</span><br><span class="line"></span><br><span class="line">    input_data = &#123;<span class="string">&quot;data&quot;</span>: data&#125;</span><br><span class="line">    responses = stub.StreamPredictions(</span><br><span class="line">        inference_pb2.PredictionsRequest(model_name=model_name, <span class="built_in">input</span>=input_data),</span><br><span class="line">        metadata=metadata,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> resp <span class="keyword">in</span> responses:</span><br><span class="line">            prediction = resp.prediction.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(prediction)</span><br><span class="line">    <span class="keyword">except</span> grpc.RpcError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;gRPC error: <span class="subst">&#123;e.details()&#125;</span>&quot;</span>)</span><br><span class="line">        exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform an advanced streaming inference with multiple input files.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer_stream2</span>(<span class="params">model_name, sequence_id, input_files, metadata</span>):</span><br><span class="line">    response_queue = queue.Queue()</span><br><span class="line">    process_response_func = partial(</span><br><span class="line">        InferStream2.default_process_response, response_queue</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    client = InferStream2SimpleClient()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        client.start_stream(</span><br><span class="line">            model_name=model_name,</span><br><span class="line">            sequence_id=sequence_id,</span><br><span class="line">            process_response=process_response_func,</span><br><span class="line">            metadata=metadata,</span><br><span class="line">        )</span><br><span class="line">        sequence = input_files.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> input_file <span class="keyword">in</span> sequence:</span><br><span class="line">            client.async_send_infer(input_file.strip())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(sequence)):</span><br><span class="line">            response = response_queue.get()</span><br><span class="line">            <span class="built_in">print</span>(<span class="built_in">str</span>(response))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Sequence completed!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> grpc.RpcError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;infer_stream2 received error&quot;</span>, e)</span><br><span class="line">        exit(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        client.stop_stream()</span><br><span class="line">        client.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Register a new model with TorchServe.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">stub, model_name, mar_set_str, metadata</span>):</span><br><span class="line">    mar_set = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">if</span> mar_set_str:</span><br><span class="line">        mar_set = <span class="built_in">set</span>(mar_set_str.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">    marfile = <span class="string">f&quot;<span class="subst">&#123;model_name&#125;</span>.mar&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;## Check <span class="subst">&#123;marfile&#125;</span> in mar_set :&quot;</span>, mar_set)</span><br><span class="line">    <span class="keyword">if</span> marfile <span class="keyword">not</span> <span class="keyword">in</span> mar_set:</span><br><span class="line">        marfile = <span class="string">&quot;https://torchserve.s3.amazonaws.com/mar_files/&#123;&#125;.mar&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            model_name</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;## Register marfile: <span class="subst">&#123;marfile&#125;</span>\n&quot;</span>)</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&quot;url&quot;</span>: marfile,</span><br><span class="line">        <span class="string">&quot;initial_workers&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&quot;synchronous&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&quot;model_name&quot;</span>: model_name,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = stub.RegisterModel(</span><br><span class="line">            management_pb2.RegisterModelRequest(**params), metadata=metadata</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Model <span class="subst">&#123;model_name&#125;</span> registered successfully&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> grpc.RpcError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Failed to register model <span class="subst">&#123;model_name&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(e.details()))</span><br><span class="line">        exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unregister a model from TorchServe.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unregister</span>(<span class="params">stub, model_name, metadata</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = stub.UnregisterModel(</span><br><span class="line">            management_pb2.UnregisterModelRequest(model_name=model_name),</span><br><span class="line">            metadata=metadata,</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Model <span class="subst">&#123;model_name&#125;</span> unregistered successfully&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> grpc.RpcError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Failed to unregister model <span class="subst">&#123;model_name&#125;</span>.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(e.details()))</span><br><span class="line">        exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The rest of the code defines the streaming classes and the command-line interface.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Argument parsing for the script</span></span><br><span class="line">    parent_parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">    parent_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;model_name&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Name of the model used.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parent_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--auth-token&quot;</span>,</span><br><span class="line">        dest=<span class="string">&quot;auth_token&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        required=<span class="literal">False</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Authorization token&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=<span class="string">&quot;TorchServe gRPC client&quot;</span>,</span><br><span class="line">        formatter_class=argparse.RawTextHelpFormatter,</span><br><span class="line">    )</span><br><span class="line">    subparsers = parser.add_subparsers(<span class="built_in">help</span>=<span class="string">&quot;Action&quot;</span>, dest=<span class="string">&quot;action&quot;</span>)</span><br><span class="line"></span><br><span class="line">    infer_action_parser = subparsers.add_parser(</span><br><span class="line">        <span class="string">&quot;infer&quot;</span>, parents=[parent_parser], add_help=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    infer_stream_action_parser = subparsers.add_parser(</span><br><span class="line">        <span class="string">&quot;infer_stream&quot;</span>, parents=[parent_parser], add_help=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    infer_stream2_action_parser = subparsers.add_parser(</span><br><span class="line">        <span class="string">&quot;infer_stream2&quot;</span>, parents=[parent_parser], add_help=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    register_action_parser = subparsers.add_parser(</span><br><span class="line">        <span class="string">&quot;register&quot;</span>, parents=[parent_parser], add_help=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line">    unregister_action_parser = subparsers.add_parser(</span><br><span class="line">        <span class="string">&quot;unregister&quot;</span>, parents=[parent_parser], add_help=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Arguments for different actions</span></span><br><span class="line">    infer_action_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;model_input&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&quot;Input for model for inference.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    infer_stream_action_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;model_input&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Input for model for stream inference.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    infer_stream2_action_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;sequence_id&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Input for sequence id for stream inference.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    infer_stream2_action_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;input_files&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Comma separated list of input files&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    register_action_parser.add_argument(</span><br><span class="line">        <span class="string">&quot;mar_set&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">        default=<span class="literal">None</span>,</span><br><span class="line">        nargs=<span class="string">&quot;?&quot;</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Comma separated list of mar models to be loaded using [model_name=]model_location format.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parse command line arguments</span></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create metadata with or without the authorization token</span></span><br><span class="line">    <span class="keyword">if</span> args.auth_token:</span><br><span class="line">        metadata = (</span><br><span class="line">            (<span class="string">&quot;protocol&quot;</span>, <span class="string">&quot;gRPC&quot;</span>),</span><br><span class="line">            (<span class="string">&quot;session_id&quot;</span>, <span class="string">&quot;12345&quot;</span>),</span><br><span class="line">            (<span class="string">&quot;authorization&quot;</span>, <span class="string">f&quot;Bearer <span class="subst">&#123;args.auth_token&#125;</span>&quot;</span>),</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        metadata = ((<span class="string">&quot;protocol&quot;</span>, <span class="string">&quot;gRPC&quot;</span>), (<span class="string">&quot;session_id&quot;</span>, <span class="string">&quot;12345&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform the selected action</span></span><br><span class="line">    <span class="keyword">if</span> args.action == <span class="string">&quot;infer&quot;</span>:</span><br><span class="line">        infer(get_inference_stub(), args.model_name, args.model_input, metadata)</span><br><span class="line">    <span class="keyword">elif</span> args.action == <span class="string">&quot;infer_stream&quot;</span>:</span><br><span class="line">        infer_stream(get_inference_stub(), args.model_name, args.model_input, metadata)</span><br><span class="line">    <span class="keyword">elif</span> args.action == <span class="string">&quot;infer_stream2&quot;</span>:</span><br><span class="line">        infer_stream2(args.model_name, args.sequence_id, args.input_files, metadata)</span><br><span class="line">    <span class="keyword">elif</span> args.action == <span class="string">&quot;register&quot;</span>:</span><br><span class="line">        register(get_management_stub(), args.model_name, args.mar_set, metadata)</span><br><span class="line">    <span class="keyword">elif</span> args.action == <span class="string">&quot;unregister&quot;</span>:</span><br><span class="line">        unregister(get_management_stub(), args.model_name, metadata)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><span style="color:red">Step 4: Run inference using a sample
client gRPC python client</span></strong>:</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  serve git:(master) ✗ python ts_scripts/torchserve_grpc_client.py infer --auth-token IMc5oeRf densenet161 examples/image_classifier/kitten.jpg</span><br><span class="line"></span><br><span class="line">/home/lvision/.local/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.2 at inference.proto. Please avoid checked-in Protobuf gencode that can be obsolete.</span><br><span class="line">  warnings.warn(</span><br><span class="line">/home/lvision/.local/lib/python3.12/site-packages/google/protobuf/runtime_version.py:112: UserWarning: Protobuf gencode version 5.27.2 is older than the runtime version 5.28.2 at management.proto. Please avoid checked-in Protobuf gencode that can be obsolete.</span><br><span class="line">  warnings.warn(</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tabby&quot;: 0.46603792905807495,</span><br><span class="line">  &quot;tiger_cat&quot;: 0.4651001989841461,</span><br><span class="line">  &quot;Egyptian_cat&quot;: 0.06611046195030212,</span><br><span class="line">  &quot;lynx&quot;: 0.001293532201088965,</span><br><span class="line">  &quot;plastic_bag&quot;: 0.000228719727601856</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CNN-PyTorch-TorchServe-Deep-Learning-Machine-Learning/" rel="tag"># CNN, PyTorch, TorchServe, Deep Learning, Machine Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/10/01/OperatingSystems/LFS/linux-from-scratch/" rel="prev" title="Linux From Scratch">
                  <i class="fa fa-angle-left"></i> Linux From Scratch
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/10/09/AI/DeepLearning/YOLOv11/" rel="next" title="YOLOv11">
                  YOLOv11 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Nobody</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
